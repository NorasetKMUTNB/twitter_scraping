{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer Key = API Key\n",
    "consumer_key= '7wlWBiho3eO8BnNQihvZptNHl'   \n",
    "# Consumer Secret = API Secret\n",
    "consumer_secret= 'd4LRXmrOOhL7V3ZdPeSHYh2QCsvt9rfJLKF9WfffTUVY1sfzoR'\n",
    "# OAuth Token = Access Token\n",
    "access_token= '824220293111500800-lO5uQ3dlEoVIQ6cjadV6JGV0iKDjGAp'\n",
    "# OAuth Token Secret = Access Token Secret\n",
    "access_token_secret= 'h2r1uSEtEp7Mo5z6tb7LLM3mbE559hD1W9v3aa8vladTZ'\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# get twiiter_API\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = [\n",
    "    '#premierleague',\n",
    "    '#laliga',\n",
    "    '#bundesliga',\n",
    "    '#thaileague',\n",
    "    '#ucl',\n",
    "    '#football',\n",
    "    '#facup',\n",
    "    '#championsleague',\n",
    "    '#seriea',\n",
    "    '#ligue1',\n",
    "    '#fathailand',\n",
    "    '#arsenal',\n",
    "    '#อาร์เซน่อล',\n",
    "    'ปืนใหญ่', \n",
    "    'ผลบอล',\n",
    "    '#manutd',\n",
    "    '#liverpool',\n",
    "    '#ronaldo',\n",
    "    '#chelsea',\n",
    "    'โรนัลโด้'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_th(txt):\n",
    "    \"\"\"Replace URLs found in a text string with nothing \n",
    "    (i.e. it will remove the URL from the string).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    txt : string\n",
    "        A text string that you want to parse and remove urls.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The same txt string with url's removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join(re.sub(\"([^\\u0E00-\\u0E7Fa-zA-Z' ]|^'|'$|''|(\\w+:\\/\\/\\S+))\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns= [\n",
    "    'keyword',\n",
    "    'language',\n",
    "    'author',\n",
    "    'twitter_name',\n",
    "    'create_at',\n",
    "    'location',\n",
    "    'text',\n",
    "    'hashtag',\n",
    "    'tweets_count',\n",
    "    'retweet_count',\n",
    "    'favourite_count',\n",
    "    'date',\n",
    "    'time',\n",
    "    'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'keyword',\n",
    "    'language',\n",
    "    'author',\n",
    "    'twitter_name',\n",
    "    'create_at', \n",
    "    'location',\n",
    "    'text', \n",
    "    'hashtag', \n",
    "    'tweets_count',\n",
    "    'retweet_count', \n",
    "    'favourite_count',\n",
    "    'date',\n",
    "    'time',\n",
    "    'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rate limit of 900 requests/15-minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_data_aday(keyword, until):\n",
    "    query = keyword\n",
    "    new_search = query + \" -filter:retweets\"\n",
    "\n",
    "    until_obj   = datetime.strptime(until, '%Y-%m-%d') + timedelta(days=1)\n",
    "    until_set   = until_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # startDate   = datetime.strptime(until+'23:59:59', '%Y-%m-%d%H:%M:%S').astimezone()\n",
    "    startDate   = datetime.strptime(until+'23:59:59+00:00', '%Y-%m-%d%H:%M:%S%z')\n",
    "    endDate     = startDate - timedelta(days=1)\n",
    "\n",
    "    df = pd.DataFrame(columns= [\n",
    "        'keyword',\n",
    "        'language',\n",
    "        'author',\n",
    "        'twitter_name',\n",
    "        'create_at',\n",
    "        'location', \n",
    "        'text', \n",
    "        'hashtag', \n",
    "        'tweets_count',\n",
    "        'retweet_count', \n",
    "        'favourite_count',\n",
    "        'date',\n",
    "        'time',\n",
    "        'sentiment'])\n",
    "\n",
    "    columns = [\n",
    "        'keyword',\n",
    "        'language',\n",
    "        'author',\n",
    "        'twitter_name',\n",
    "        'create_at', \n",
    "        'location',\n",
    "        'text', \n",
    "        'hashtag', \n",
    "        'tweets_count',\n",
    "        'retweet_count', \n",
    "        'favourite_count',\n",
    "        'date',\n",
    "        'time',\n",
    "        'sentiment']\n",
    "\n",
    "    ############################################################\n",
    "    # twitter(TH)\n",
    "    ############################################################\n",
    "    for tweet in tw.Cursor(api.search_tweets,\n",
    "        q=new_search,\n",
    "        lang='th',\n",
    "        until=until_set,\n",
    "        result_type='recent',\n",
    "        tweet_mode='extended').items(1000):\n",
    "\n",
    "        # create_at = tweet.created_at.astimezone()\n",
    "        create_at = tweet.created_at\n",
    "\n",
    "        if create_at > startDate: continue\n",
    "        elif create_at < endDate : break\n",
    "\n",
    "        # keyword = query\n",
    "        language = 'th'\n",
    "        tweets_count = 1\n",
    "\n",
    "        # hashtag\n",
    "        entity_hashtag = tweet.entities.get('hashtags')\n",
    "        hashtag = ''\n",
    "        for i in range(0,len(entity_hashtag)):\n",
    "            hashtag = hashtag +'/'+entity_hashtag[i]['text']\n",
    "\n",
    "        # infomantion\n",
    "        twitter_name = '@'+tweet.user.screen_name\n",
    "        author = tweet.user.name\n",
    "        location = tweet.user.location\n",
    "        re_count = tweet.retweet_count\n",
    "        tweets_count += re_count\n",
    "        \n",
    "        date = create_at.strftime(\"%d/%m/%Y\")\n",
    "        time = create_at.strftime(\"%H:%M\")\n",
    "\n",
    "        try:\n",
    "            text = tweet.retweeted_status.full_text\n",
    "            fav_count = tweet.retweeted_status.favorite_count\n",
    "        except:\n",
    "            text = tweet.full_text\n",
    "            fav_count = tweet.favorite_count\n",
    "\n",
    "        sentiment = senti.checksentimentword(remove_url_th(text))\n",
    "\n",
    "        # temp for a tweet in data_frame\n",
    "        new_column = pd.DataFrame([[\n",
    "            keyword, \n",
    "            language, \n",
    "            author,\n",
    "            twitter_name,\n",
    "            create_at, \n",
    "            location,\n",
    "            text, \n",
    "            hashtag, \n",
    "            tweets_count, \n",
    "            re_count,\n",
    "            fav_count, \n",
    "            date,\n",
    "            time,\n",
    "            sentiment]], columns = columns)\n",
    "\n",
    "        # append in data_frame\n",
    "        df = pd.concat([df,new_column],ignore_index=True)\n",
    "\n",
    "    ############################################################\n",
    "    # twitter(EN)\n",
    "    ############################################################\n",
    "    for tweet in tw.Cursor(api.search_tweets,\n",
    "        q=new_search,\n",
    "        lang='en',\n",
    "        until=until_set,\n",
    "        result_type='recent',\n",
    "        tweet_mode='extended').items(1000):\n",
    "\n",
    "        # create_at = tweet.created_at.astimezone()\n",
    "        create_at = tweet.created_at\n",
    "\n",
    "        if create_at > startDate: continue\n",
    "        elif create_at < endDate : break\n",
    "        \n",
    "        keyword = query\n",
    "        language = 'en'\n",
    "        tweets_count = 1\n",
    "\n",
    "        # hashtag\n",
    "        entity_hashtag = tweet.entities.get('hashtags')\n",
    "        hashtag = ''\n",
    "        for i in range(0,len(entity_hashtag)):\n",
    "            hashtag = hashtag +'/'+entity_hashtag[i]['text']\n",
    "\n",
    "        # infomantion\n",
    "        twitter_name = '@'+tweet.user.screen_name\n",
    "        author = tweet.user.name\n",
    "        location = tweet.user.location\n",
    "        re_count = tweet.retweet_count\n",
    "        tweets_count += re_count\n",
    "\n",
    "        date = create_at.strftime(\"%d/%m/%Y\")\n",
    "        time = create_at.strftime(\"%H:%M\")\n",
    "\n",
    "        try:\n",
    "            text = tweet.retweeted_status.full_text\n",
    "            fav_count = tweet.retweeted_status.favorite_count\n",
    "        except:\n",
    "            text = tweet.full_text\n",
    "            fav_count = tweet.favorite_count\n",
    "\n",
    "        sentiment = senti.checksentimentword(remove_url_th(text))\n",
    "\n",
    "        new_column = pd.DataFrame([[\n",
    "            keyword, \n",
    "            language, \n",
    "            author,\n",
    "            twitter_name,\n",
    "            create_at, \n",
    "            location,\n",
    "            text, \n",
    "            hashtag, \n",
    "            tweets_count, \n",
    "            re_count,\n",
    "            fav_count, \n",
    "            date,\n",
    "            time,\n",
    "            sentiment]], columns = columns)\n",
    "\n",
    "        # append in data_frame\n",
    "        df = pd.concat([df,new_column],ignore_index=True)\n",
    "\n",
    "    # convent to csv\n",
    "    df.to_csv('backup//{}//file_date//{}_{}_twitterCrawler.csv'.format(keyword, keyword, until), index=False, encoding='utf-8')\n",
    "    print('finished : {} at {}'.format(keyword, until))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = '#ucl'\n",
    "# until='2022-03-22'\n",
    "# new_data_aday(key,until)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#premierleague\n",
      "finished : #premierleague at 2022-05-01\n",
      "#laliga\n",
      "finished : #laliga at 2022-05-01\n",
      "#bundesliga\n",
      "finished : #bundesliga at 2022-05-01\n",
      "#thaileague\n",
      "finished : #thaileague at 2022-05-01\n",
      "#ucl\n",
      "finished : #ucl at 2022-05-01\n",
      "#football\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished : #football at 2022-05-01\n",
      "#facup\n",
      "finished : #facup at 2022-05-01\n",
      "#championsleague\n",
      "finished : #championsleague at 2022-05-01\n",
      "#seriea\n",
      "finished : #seriea at 2022-05-01\n",
      "#ligue1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished : #ligue1 at 2022-05-01\n",
      "#fathailand\n",
      "finished : #fathailand at 2022-05-01\n",
      "#arsenal\n",
      "finished : #arsenal at 2022-05-01\n",
      "#อาร์เซน่อล\n",
      "finished : #อาร์เซน่อล at 2022-05-01\n",
      "ปืนใหญ่\n",
      "finished : ปืนใหญ่ at 2022-05-01\n",
      "ผลบอล\n",
      "finished : ผลบอล at 2022-05-01\n",
      "#manutd\n",
      "finished : #manutd at 2022-05-01\n",
      "#liverpool\n",
      "finished : #liverpool at 2022-05-01\n",
      "#ronaldo\n",
      "finished : #ronaldo at 2022-05-01\n",
      "#chelsea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished : #chelsea at 2022-05-01\n",
      "โรนัลโด้\n",
      "finished : โรนัลโด้ at 2022-05-01\n"
     ]
    }
   ],
   "source": [
    "for key in keyword :\n",
    "    print(key)\n",
    "    # create_directory(key)\n",
    "    until='2022-05-01'\n",
    "    new_data_aday(key,until)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### union file date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#premierleague finished\n",
      "#laliga finished\n",
      "#bundesliga finished\n",
      "#thaileague finished\n",
      "#ucl finished\n",
      "#football finished\n",
      "#facup finished\n",
      "#championsleague finished\n",
      "#seriea finished\n",
      "#ligue1 finished\n",
      "#fathailand finished\n",
      "#arsenal finished\n",
      "#อาร์เซน่อล finished\n",
      "ปืนใหญ่ finished\n",
      "ผลบอล finished\n",
      "#manutd finished\n",
      "#liverpool finished\n",
      "#ronaldo finished\n",
      "#chelsea finished\n",
      "โรนัลโด้ finished\n"
     ]
    }
   ],
   "source": [
    "start_date  = '2022-03-22'\n",
    "end_date    = '2022-03-28'\n",
    "# end_date to datetime obj\n",
    "end_date_obj    = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "for key in keyword:\n",
    "    # key = '#premierleague'\n",
    "    i = 0\n",
    "\n",
    "    df = pd.DataFrame(columns= [\n",
    "        'keyword',\n",
    "        'language',\n",
    "        'author',\n",
    "        'twitter_name',\n",
    "        'create_at',\n",
    "        'location', \n",
    "        'text', \n",
    "        'hashtag', \n",
    "        'tweets_count',\n",
    "        'retweet_count',\n",
    "        'favourite_count',\n",
    "        'date',\n",
    "        'time',\n",
    "        'sentiment'])\n",
    "\n",
    "    while True:\n",
    "        until_obj   = datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=i)\n",
    "        date        = until_obj.strftime(\"%Y-%m-%d\") # str\n",
    "\n",
    "        # break out loop\n",
    "        if until_obj > end_date_obj:break\n",
    "\n",
    "        temp_df = pd.read_csv('backup//{}//file_date//{}_{}_twitterCrawler.csv'.format(key ,key, date))\n",
    "        df = pd.concat([df,temp_df],ignore_index=True)\n",
    "        i+=1\n",
    "\n",
    "    # convent to csv\n",
    "    df.to_csv('backup//{}//{}.csv'.format(key, key), index=False, encoding='utf-8')\n",
    "    print(key+' finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_file(key):\n",
    "        path = \".//backup//{}//file_date\".format(key)\n",
    "\n",
    "        df = pd.DataFrame(columns= [\n",
    "            'keyword',\n",
    "            'language',\n",
    "            'author',\n",
    "            'twitter_name',\n",
    "            'create_at',\n",
    "            'location', \n",
    "            'text', \n",
    "            'hashtag', \n",
    "            'tweets_count',\n",
    "            'retweet_count',\n",
    "            'favourite_count',\n",
    "            'date',\n",
    "            'time',\n",
    "            'sentiment'])\n",
    "\n",
    "        list_date = []\n",
    "\n",
    "        for x in os.listdir(path):\n",
    "            if x.endswith(\".csv\"):\n",
    "                # Prints only text file present in My Folder\n",
    "                x = x.replace('.csv', '')\n",
    "                # print(x)\n",
    "                list_date.append(x.split(\"_\")[1])\n",
    "\n",
    "                temp_df = pd.read_csv('.//backup//{}//file_date//{}.csv'.format(key ,x))\n",
    "                df = pd.concat([df,temp_df],ignore_index=True)\n",
    "\n",
    "                # convent to csv\n",
    "                df.to_csv('.//backup//{}//{}.csv'.format(key, key), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keyword:\n",
    "    union_file(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convent to csv\n",
    "# df.to_csv('{}.csv'.format(key), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collcet file.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-22\n",
      "2022-03-23\n",
      "2022-03-24\n",
      "2022-03-25\n",
      "2022-03-26\n",
      "2022-03-27\n",
      "2022-03-28\n",
      "2022-03-29\n",
      "2022-03-30\n",
      "2022-03-31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"C://yaengg//DNS//tweepy//backup//#arsenal//file_date\"\n",
    "\n",
    "for x in os.listdir(path):\n",
    "    if x.endswith(\".csv\"):\n",
    "        # Prints only text file present in My Folder\n",
    "        x = x.replace('.csv', '')\n",
    "        # print(x)\n",
    "        print(x.split(\"_\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### counting word&hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "\n",
    "# b = TextBlob(\"bonjour\")\n",
    "b = \"bonjour\"\n",
    "detect(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "from pythainlp.util import isthai\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "def cleanText(text):\n",
    "  text = remove_url_th(text).lower()\n",
    "  # check language\n",
    "  if isthai(text):\n",
    "    text = remove_url_th(text)\n",
    "    text = re.sub('[^ก-๙]','',text)\n",
    "    stop_word = list(thai_stopwords())\n",
    "    sentence = word_tokenize(text, engine=\"newmm\")\n",
    "    result = [word for word in sentence if word not in stop_word and \" \" not in word]\n",
    "    return \"/\".join(result)\n",
    "  else:\n",
    "    stop_word = set(stopwords.words('english'))\n",
    "    sentence = text.split()\n",
    "    result = [word for word in sentence if word not in stop_word and  \" \" not in word]\n",
    "    return \"/\".join(result)\n",
    "\n",
    "\n",
    "def tokenize(d):\n",
    "  result = d.split(\"/\")\n",
    "  result = list(filter(None, result))\n",
    "  return result\n",
    "\n",
    "\n",
    "def language(data):\n",
    "  language = pd.Series([],dtype=pd.StringDtype())\n",
    "  for i in range(len(data)):\n",
    "    word = data[\"word\"][i] \n",
    "    if isthai(word): \n",
    "      language[i]=\"th\"\n",
    "    else: \n",
    "      language[i]=\"en\"\n",
    "  data.insert(1, \"language\", language)\n",
    "\n",
    "\n",
    "def BoW_tweet(key):\n",
    "  df = pd.read_csv('backup//{}//{}.csv'.format(key, key))\n",
    "\n",
    "  new_text = []\n",
    "  for txt in df[\"text\"]:\n",
    "    new_text.append(cleanText(txt))\n",
    "\n",
    "  keyword_df = pd.DataFrame(columns = ['word', 'count_word'])\n",
    "\n",
    "  vectorizer = CountVectorizer(tokenizer=tokenize)   \n",
    "  transformed_data = vectorizer.fit_transform(new_text)\n",
    "  keyword_df['word'] = vectorizer.get_feature_names_out()\n",
    "\n",
    "  # inserting column with static value in data frame\n",
    "  keyword_df.insert(0,'keyword',key)\n",
    "  language(keyword_df)\n",
    "\n",
    "  # counting of word\n",
    "  keyword_df['count_word'] = np.ravel(transformed_data.sum(axis=0))\n",
    "\n",
    "  # keyword_df2.sort_values(by=['count_word'], ascending=False).head(10)\n",
    "  keyword_df = keyword_df.sort_values(by=['count_word'], ascending=False)\n",
    "  # keyword_df\n",
    "  keyword_df.to_csv('backup//{}//{}_count_word.csv'.format(key, key), index=False, encoding='utf-8')\n",
    "  print('{} word finished'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "def slash_tokenize(d):  \n",
    "  result = d.split(\"/\")\n",
    "  result.remove('')\n",
    "  return result\n",
    "\n",
    "def count_hashtag(key):\n",
    "  df = pd.read_csv('backup//{}//{}.csv'.format(key, key))\n",
    "\n",
    "  hastag_data = df[\"hashtag\"].dropna()\n",
    "\n",
    "  vectorizer = CountVectorizer(tokenizer=slash_tokenize)\n",
    "  transformed_data = vectorizer.fit_transform(hastag_data)\n",
    "\n",
    "  hash_tag_cnt_df = pd.DataFrame(columns = ['hashtag', 'count']) \n",
    "  hash_tag_cnt_df['hashtag'] = vectorizer.get_feature_names_out()\n",
    "  hash_tag_cnt_df.insert(0,'keyword',key)\n",
    "\n",
    "  hash_tag_cnt_df['count'] = np.ravel(transformed_data.sum(axis=0))\n",
    "\n",
    "  # hash_tag_cnt_df.sort_values(by=['count'], ascending=False).head(10)\n",
    "  hash_tag_cnt_df = hash_tag_cnt_df.sort_values(by=['count'], ascending=False)\n",
    "\n",
    "  hash_tag_cnt_df.to_csv('backup//{}//{}_count_hashtag.csv'.format(key, key), index=False, encoding='utf-8')\n",
    "  print('{} hashtag finished'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#premierleague hashtag finished\n",
      "#premierleague word finished\n",
      "\n",
      "\n",
      "#laliga hashtag finished\n",
      "#laliga word finished\n",
      "\n",
      "\n",
      "#bundesliga hashtag finished\n",
      "#bundesliga word finished\n",
      "\n",
      "\n",
      "#thaileague hashtag finished\n",
      "#thaileague word finished\n",
      "\n",
      "\n",
      "#ucl hashtag finished\n",
      "#ucl word finished\n",
      "\n",
      "\n",
      "#football hashtag finished\n",
      "#football word finished\n",
      "\n",
      "\n",
      "#facup hashtag finished\n",
      "#facup word finished\n",
      "\n",
      "\n",
      "#championsleague hashtag finished\n",
      "#championsleague word finished\n",
      "\n",
      "\n",
      "#seriea hashtag finished\n",
      "#seriea word finished\n",
      "\n",
      "\n",
      "#ligue1 hashtag finished\n",
      "#ligue1 word finished\n",
      "\n",
      "\n",
      "#fathailand hashtag finished\n",
      "#fathailand word finished\n",
      "\n",
      "\n",
      "#arsenal hashtag finished\n",
      "#arsenal word finished\n",
      "\n",
      "\n",
      "#อาร์เซน่อล hashtag finished\n",
      "#อาร์เซน่อล word finished\n",
      "\n",
      "\n",
      "ปืนใหญ่ hashtag finished\n",
      "ปืนใหญ่ word finished\n",
      "\n",
      "\n",
      "ผลบอล hashtag finished\n",
      "ผลบอล word finished\n",
      "\n",
      "\n",
      "#manutd hashtag finished\n",
      "#manutd word finished\n",
      "\n",
      "\n",
      "#liverpool hashtag finished\n",
      "#liverpool word finished\n",
      "\n",
      "\n",
      "#ronaldo hashtag finished\n",
      "#ronaldo word finished\n",
      "\n",
      "\n",
      "#chelsea hashtag finished\n",
      "#chelsea word finished\n",
      "\n",
      "\n",
      "โรนัลโด้ hashtag finished\n",
      "โรนัลโด้ word finished\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# key = '#arsenal'\n",
    "for key in keyword:\n",
    "    union_file(key)\n",
    "    count_hashtag(key)\n",
    "    BoW_tweet(key)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### search directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#arsenal', '#bundesliga', '#championsleague', '#chelsea', '#facup', '#fathailand', '#football', '#laliga', '#ligue1', '#liverpool', '#manutd', '#premierleague', '#ronaldo', '#seriea', '#thaileague', '#ucl', '#อาร์เซน่อล', 'yoghurtbnk48', 'ปืนใหญ่', 'ผลบอล', 'โรนัลโด้']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder = './backup'\n",
    "list_keyword = [name for name in os.listdir(folder) if os.path.isdir(os.path.join(folder, name))]\n",
    "\n",
    "print(list_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(key):\n",
    "    # Directory\n",
    "    directory = key\n",
    "    # Parent Directory path\n",
    "    parent_dir = \"C://yaengg\\DNS//tweepy//backup\"\n",
    "    \n",
    "    # Path\n",
    "    path = os.path.join(parent_dir, directory)\n",
    "    \n",
    "    # Create the directory\n",
    "    # directory in parent_dir\n",
    "    os.mkdir(path)\n",
    "\n",
    "    # Create the directory\n",
    "    # directory in parent_dir\n",
    "    date_path = os.path.join(parent_dir, directory, 'file_date')\n",
    "    os.mkdir(date_path)\n",
    "\n",
    "    print(\"Directory '% s' created\" %directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### union file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def union_file(key):\n",
    "    path = \"C://yaengg//DNS//tweepy//backup//{}//file_date\".format(key)\n",
    "\n",
    "    df = pd.DataFrame(columns= [\n",
    "        'keyword',\n",
    "        'language',\n",
    "        'author',\n",
    "        'twitter_name',\n",
    "        'create_at',\n",
    "        'location', \n",
    "        'text', \n",
    "        'hashtag', \n",
    "        'tweets_count',\n",
    "        'retweet_count',\n",
    "        'favourite_count',\n",
    "        'date',\n",
    "        'time',\n",
    "        'sentiment'])\n",
    "\n",
    "    for x in os.listdir(path):\n",
    "        if x.endswith(\".csv\"):\n",
    "            # Prints only text file present in My Folder\n",
    "            x = x.replace('.csv', '')\n",
    "            # print(x)\n",
    "\n",
    "            temp_df = pd.read_csv('backup//{}//file_date//{}.csv'.format(key ,x))\n",
    "            df = pd.concat([df,temp_df],ignore_index=True)\n",
    "\n",
    "            # convent to csv\n",
    "            df.to_csv('backup//{}//{}.csv'.format(key, key), index=False, encoding='utf-8')\n",
    "            print(key+' union finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serach Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today  2022-04-04  unitl  2022-03-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "today = datetime.today().now().date()\n",
    "until_date_str = (today - timedelta(days=7)).isoformat()\n",
    "until_date = datetime.strptime(until_date_str, '%Y-%m-%d')\n",
    "print('today ',today,' unitl ', until_date)\n",
    "\n",
    "# collect keywords which have been searched\n",
    "folder = './backup'\n",
    "list_keyword = [name for name in os.listdir(folder) if os.path.isdir(os.path.join(folder, name))]\n",
    "# print(list_keyword)\n",
    "\n",
    "keyword = input('Your Keyword : ').lower()\n",
    "# check keyword\n",
    "if keyword in list_keyword:\n",
    "    # have been search\n",
    "    print(df.head(5))\n",
    "    df = pd.read_csv('backup//{}//{}.csv'.format(keyword, keyword))\n",
    "else:\n",
    "    # new keyword for search\n",
    "    analog = input(\"it's never search, do you want to search?(Y/N): \").lower()\n",
    "    if analog == 'y': \n",
    "        start_date = input(\"What is your start date you want?({}): \".format(today.strftime(\"%Y-%m-%d\")))\n",
    "        limit_date = (datetime.strptime(start_date, '%Y-%m-%d') - until_date).days\n",
    "        print('date ',start_date,' limit_date ',limit_date)\n",
    "        while True : \n",
    "            num_date = int(input(\"How many days do you want to go back?(limit:{}): \".format(limit_date)))\n",
    "            if num_date <= limit_date: break\n",
    "        \n",
    "        # create new directory \n",
    "        create_directory(keyword)\n",
    "\n",
    "        start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        if num_date == 0: new_data_aday(keyword, start_date_obj)\n",
    "        else:\n",
    "            print('enddate : {}'.format(datetime.strptime(start_date, '%Y-%m-%d') - timedelta(days=num_date)))\n",
    "            \n",
    "            # file_date\n",
    "            for i in range(num_date+1):\n",
    "                until_obj   = datetime.strptime(start_date, '%Y-%m-%d') - timedelta(days=i)\n",
    "                tmep_date   = until_obj.strftime(\"%Y-%m-%d\") # str\n",
    "                new_data_aday(keyword, tmep_date)\n",
    "\n",
    "            # union file_date\n",
    "            union_file(keyword)\n",
    "\n",
    "            # collect counting\n",
    "            BoW_tweet(keyword)\n",
    "            count_hashtag(keyword)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dcacb0086e9a4f4eabd41c33bf4faac5ea0a3337ed3f5eff0680afa930572c04"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
